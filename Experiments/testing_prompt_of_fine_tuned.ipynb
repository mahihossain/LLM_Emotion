{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import (\n",
    "    PeftConfig,\n",
    "    PeftModel\n",
    ")\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f2bded1eb9344708cce212dc1f68b1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<human>: midjourney prompt for a boy running in the snow\n",
      "<assistant>: Boy running in the snow, snow-covered field, matte painting, Cinematic, 8k, hyper-realistic, photorealistic --ar 16:9 --uplight --no dof --octane render --bitmooth --statue of liberty in foreground --trending on Artstation --shapemating --ar 16:9 --cubism --geometric --muted colors --ar 16:9 --photorealistic --atmosphere --matthew wilder --m Machine learning --sharpen --res 8k --no volumetric lighting --octane -- 8k --houdini --ar 16:9 --high detailed --realistic --atmosphere --lighting --mood --night light --warm light --artistic --octane render --ar 16:9 --no fog --ar 16:\n"
     ]
    }
   ],
   "source": [
    "PEFT_MODEL = \"./models/llama-2-7b-chat-hf-midjourney-finetuned-peft/\"\n",
    "\n",
    "config = PeftConfig.from_pretrained(PEFT_MODEL)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    return_dict=True,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = PeftModel.from_pretrained(model, PEFT_MODEL)\n",
    "\n",
    "# %%\n",
    "generation_config = model.generation_config\n",
    "generation_config.max_new_tokens = 200\n",
    "generation_config.temperature = 0.7\n",
    "generation_config.top_p = 0.7\n",
    "generation_config.num_return_sequences = 1\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# %%\n",
    "device = \"cuda:0\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "<human>: midjourney prompt for a boy running in the snow\n",
    "<context>: \"There are 8 classes of non-verbal behaviour with sub-classes that you can use in your analysis. In brackets I give a short explanation of the meaning of each behaviour:\n",
    "- Gaze: DOWN (looking down), DOWN_LEFT (looking down left), DOWN_RIGHT (looking down right), LEFT (looking left), RIGHT (looking right), STRAIGHT AT INTERVIEWER (looking straight at interviewer), UP (looking up), UP_LEFT (looking up left), UP_RIGHT (looking up right).\n",
    "- Eyes: PINCH (eyes are squeezed together), BLINK_REPEATEDLY (person blinks more than once in a row), CLOSE (eyes are closed), WIDEN (person widens eyes)\n",
    "- Ekman expression: FEAR (the person shows the typical characteristics of the Ekman expression fear), DISGUST (the person shows the typical characteristics of the Ekman expression disgust), JOY (the person shows the typical characteristics of the Ekman expression joy), SURPRISE (the person shows the typical characteristics of the Ekman expression surprise)\n",
    "- Smile: DUCHENNE (a smile that reaches the eyes), NON_DUCHENNE (a smile that concentrates only on the mouth)\n",
    "- Head: SHAKE (the head is moved alternately to the right and left), NOD (the head is moved down and then up), DOWN_RIGHT (the head is moved down to the right), DOWN_LEFT (the head is moved down to the left), DOWN (the head is moved down), LEFT (the head is moved to the left), RIGHT (the head is moved to the right), STRAIGHT (the head is held straight), UP (the head is moved up)\n",
    "- Head Tilt: TILT (the head is tilted to the side)\n",
    "-UpperBody: BACKWARD (the upper body is moved backwards), FORWARD (the upper body is moved forwards), DIRECTED_AWAY (the person turns away), SHRUG (the person moves the shoulders up and down), SIDEAWAYS (the person moves the body to the left or right without turning), SLUMP (the person slumps down), STRAIGHT (the person stays straight)\n",
    "Speech: SPEECH (the person is speaking), FILLER (the person is saying filler words), BREATH (the person is breathing heavily), LAUGHTER (the person is laughing).\n",
    " \n",
    "For all 8 classes of non-verbal behaviour, the subclass REST is also possible. This indicates that none of the other subclasses were detected.\n",
    "\n",
    "In addition to the non-verbal behaviour, each row contains information about the person, the persons introspection and the context:\n",
    "1. The subject who performed the behaviour (column named 'session'). The 'session' column contains 10 different subjects. VPN01 in the table corresponds to 'Versuchsperson 1' in the document '2deep_session' and VPN02 corresponds to 'Versuchsperson 2' and so on.\n",
    "2. The situation in which it occurred (column called 'Situation'). In the table, the value 'situation 1' has been abbreviated to 'S1' and the value 'situation 2' to 'S2'. \n",
    "3. The mindedness of the person (column called 'MindednessMean'). Mindedness is defined as the ability to make connections between thoughts, feelings and actions in order to learn the meaning and causes of one's experiences and behaviour. Higher scores in this column indicate a greater ability to be mindful.\n",
    "4. The person's gender (column called 'Gender').\n",
    "5. The introspection of the person. This includes the following columns:\n",
    "- ExperiencedEmotion1 and ExperiencedEmotion2- describes the experienced emotions\n",
    "- ShameAwarenessSituation - 'yes' means that the person was aware of feeling ashamed in the situation, 'no' means that the person was not aware of feeling ashamed\n",
    "- ShameAwarenessInterview - 'yes' means that the person was aware of feeling ashamed in a later interview, 'no' means that the person was not aware of feeling ashamed\n",
    "- RuleDisplay - display rules are rules that define how emotions are communicated; the value \"Display_Rule\" mean that a display rule has been applied.\n",
    "- RelationshipIntention - a person can have the intention to maintain or terminate the relationship with the interviewer, if both intentions are existing it is labeled as 'unclear'\n",
    "- InternalEmotion - following internal emotions are possible: REST (none of the following emotions could be detected), SHAME_SHYNESS, UNCLEAR (it is not clear which internal emotion it is), ENJOYMENT, GPA (=general positive affect), SURPRISE\n",
    "- ShameDisplay - can be open or not open (regulated shame)\n",
    "\n",
    "- WITHDRAWAL (The person acknowledges the experience as negative, accepts shame’s message as valid, and tries to withdraw or hide from the situation. The phenomenological experience is negative; emotions include shame, sadness, fear, and anxiety. )\n",
    "\n",
    "- ATTACK_OTHER (The person may – or may not – acknowledge the negative experience of self, typically does not accept shame’s message, and attempts are made to make someone else feel worse. The phenomenological experience is negative; anger is directed outward, perhaps toward the source of the shaming event.)\n",
    "\n",
    "- ATTACK_SELF (The person acknowledges the experience as negative, accepts shame’s message as valid, and turns anger inward. The phenomenological experience is negative; emotions include self-directed anger, contempt, or disgust, which magnify the impact of shame.)\n",
    "\n",
    "- AVOIDANCE (The person typically does not acknowledge the negative experience of self, typically does not accept shame’s message as valid (denial), and attempts are made to distract the self and others from the painful feeling. The phenomenological experience becomes neutral or positive; shame may be disavowed, or overridden with joy or excitement via distractions (e.g., sex).)\n",
    "\n",
    "- DEPRECIATION (devaluation of the other due to different values)\n",
    "\n",
    "- STABILIZE_SELF (focus on positive selfconcept,\n",
    "act compliant with self-concept)\n",
    "\n",
    "- REST (no emotion regulation strategy could be identified)\"\n",
    "<assistant>:\n",
    "\"\"\".strip()\n",
    "\n",
    "encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "with torch.inference_mode():\n",
    "  outputs = model.generate(\n",
    "      input_ids = encoding.input_ids,\n",
    "      attention_mask = encoding.attention_mask,\n",
    "      generation_config = generation_config\n",
    "  )\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
