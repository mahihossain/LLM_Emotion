{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import (\n",
    "    PeftConfig,\n",
    "    PeftModel\n",
    ")\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the test data\n",
    "df = pd.read_csv('../data/test_person_10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PEFT_MODEL = \"./models/llama-2-7b-chat-hf-llm-emo-person-10-finetuned-peft/\"\n",
    "\n",
    "config = PeftConfig.from_pretrained(PEFT_MODEL)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    return_dict=True,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = PeftModel.from_pretrained(model, PEFT_MODEL)\n",
    "\n",
    "# %%\n",
    "generation_config = model.generation_config\n",
    "generation_config.max_new_tokens = 200\n",
    "generation_config.temperature = 0.7\n",
    "generation_config.top_p = 0.7\n",
    "generation_config.num_return_sequences = 1\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# if cuda is available, use it\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# prompt = \"\"\"\n",
    "# <human>: midjourney prompt for a boy running in the snow\n",
    "# <context>: \n",
    "# <assistant>:\n",
    "# \"\"\".strip()\n",
    "\n",
    "# encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "# with torch.inference_mode():\n",
    "#   outputs = model.generate(\n",
    "#       input_ids = encoding.input_ids,\n",
    "#       attention_mask = encoding.attention_mask,\n",
    "#       generation_config = generation_config\n",
    "#   )\n",
    "\n",
    "# print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = ['REST', 'DEPRECIATION', 'AVOIDANCE', 'STABILIZE_SELF',\n",
    "       'ATTACK_OTHER', 'WITHDRAWAL', 'ATTACK_SELF']\n",
    "df_copy = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over the dataset, User column is <human> and <context> is the context and <assistant> is the model response\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    prompt = f\"\"\"\n",
    "    <human>: {row['User']}\n",
    "    <context>: {row['context']}\n",
    "    <assistant>:\n",
    "    \"\"\".strip()\n",
    "\n",
    "    encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            input_ids = encoding.input_ids,\n",
    "            attention_mask = encoding.attention_mask,\n",
    "            generation_config = generation_config\n",
    "        )\n",
    "\n",
    "    # Decode the assistant's response\n",
    "    assistant_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract the part of the assistant's response that comes after '<assistant>'\n",
    "    assistant_response = assistant_response.split('<assistant>')[-1].strip().lower()\n",
    "\n",
    "    # Check if the assistant's response contains a valid emotion\n",
    "    emotion_counter = Counter()\n",
    "    for emotion in emotions:\n",
    "        emotion_counter[emotion] = assistant_response.count(emotion.lower())\n",
    "\n",
    "    if emotion_counter:\n",
    "        # If a valid emotion is found and it's the same as the ground truth, keep the ground truth in df_copy\n",
    "        most_common_emotion = emotion_counter.most_common(1)[0][0]\n",
    "        if row['EmotionRegulation1'].lower() in [emotion for emotion, count in emotion_counter.items() if count > 0]:\n",
    "            df_copy.loc[index, 'EmotionRegulation1'] = row['EmotionRegulation1']\n",
    "        else:\n",
    "            # If a valid emotion is found but it's not the same as the ground truth, replace the ground truth with the most common emotion in df_copy\n",
    "            df_copy.loc[index, 'EmotionRegulation1'] = most_common_emotion\n",
    "    else:\n",
    "        # If no valid emotion is found, put 'REST' in df_copy\n",
    "        df_copy.loc[index, 'EmotionRegulation1'] = 'REST'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all the columns except for the 'EmotionRegulation1' column\n",
    "df_copy = df_copy['EmotionRegulation1']\n",
    "\n",
    "# save the df_copy to a csv file as prediciton_peroosn_10.csv\n",
    "df_copy.to_csv('../data/prediction_person_10.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
