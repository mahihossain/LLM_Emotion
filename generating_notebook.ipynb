{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig, pipeline\n",
    "import time\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftConfig,\n",
    "    PeftModel,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_bit_llama = \"./models/llama-2-70b-hf-quantized-3bits/\"\n",
    "four_bit_llama = \"./models/llama-2-70b-hf-quantized-4bits/\"\n",
    "leader_board_model = \"rwitz2/go-bruins-v2.1.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54edb3076f9440ce9bbd4455f0a69fc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(four_bit_llama, device_map=\"auto\", cache_dir=\"./models\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(four_bit_llama, cache_dir=\"./models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***CHAT***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML, clear_output, Markdown\n",
    "import textwrap\n",
    "import ipywidgets as widgets\n",
    "import re\n",
    "import io\n",
    "import pandas as pd\n",
    "import json\n",
    "import nbformat\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "runtimeFlag = \"cuda:0\" #Run on GPU (you can't run GPTQ on cpu)\n",
    "scaling_factor = 1.0 # allows for a max sequence length of 16384*6 = 98304! Unfortunately, requires Colab Pro and a V100 or A100 to have sufficient RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful assistant.\n"
     ]
    }
   ],
   "source": [
    "# Set the SYSTEM PROMPT\n",
    "# DEFAULT_SYSTEM_PROMPT = 'You are a helpful pair-coding assistant.'\n",
    "DEFAULT_SYSTEM_PROMPT = 'You are a helpful assistant.'\n",
    "SYSTEM_PROMPT = DEFAULT_SYSTEM_PROMPT\n",
    "\n",
    "print(SYSTEM_PROMPT)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "\n",
    "B_INST, E_INST = \"Question: \", \"Answer: \"\n",
    "\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "\n",
    "scaling_factor = 1.0\n",
    "max_context = int(model.config.max_position_embeddings*scaling_factor)\n",
    "max_doc_length = int(0.75 * max_context)  # max doc length is 75% of the context length\n",
    "max_doc_words = int(max_doc_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(dialogs, temperature=0.01, top_p=0.9, logprobs=False):\n",
    "    torch.cuda.empty_cache()\n",
    "    # print(json.dumps(dialogs, indent=4))\n",
    "    max_prompt_len = int(0.85 * max_context)\n",
    "    max_gen_len = int(0.10 * max_prompt_len)\n",
    "\n",
    "    prompt_tokens = []\n",
    "    for dialog in dialogs:\n",
    "        if dialog[0][\"role\"] != \"system\":\n",
    "            dialog = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": SYSTEM_PROMPT,\n",
    "                }\n",
    "            ] + dialog\n",
    "        dialog_tokens = [tokenizer(\n",
    "            # f\"{B_INST} {B_SYS}{(dialog[0]['content']).strip()}{E_SYS}{(dialog[1]['content']).strip()} {E_INST}\",\n",
    "            f\"{B_INST} {(dialog[1]['content']).strip()} {E_INST}\", # Omits the system prompt altogether\n",
    "            return_tensors=\"pt\",\n",
    "            add_special_tokens=True\n",
    "        ).input_ids.to(runtimeFlag)]\n",
    "        for i in range(2, len(dialog), 2):\n",
    "            user_tokens = tokenizer(\n",
    "                f\"{B_INST} {(dialog[i+1]['content']).strip()} {E_INST}\",\n",
    "                return_tensors=\"pt\",\n",
    "                add_special_tokens=True\n",
    "            ).input_ids.to(runtimeFlag)\n",
    "            assistant_w_eos = dialog[i]['content'].strip() + tokenizer.eos_token\n",
    "            assistant_tokens = tokenizer(\n",
    "                            assistant_w_eos,\n",
    "                            return_tensors=\"pt\",\n",
    "                            add_special_tokens=False\n",
    "                        ).input_ids.to(runtimeFlag)\n",
    "            tokens = torch.cat([assistant_tokens, user_tokens], dim=-1)\n",
    "            dialog_tokens.append(tokens)\n",
    "        prompt_tokens.append(torch.cat(dialog_tokens, dim=-1))\n",
    "\n",
    "    input_ids = prompt_tokens[0]\n",
    "    if len(input_ids[0]) > max_prompt_len:\n",
    "        return \"\\n\\n **The language model's input limit has been reached. Clear the chat and start afresh!**\"\n",
    "\n",
    "    # print(tokenizer.decode(input_ids[0], skip_special_tokens=True))\n",
    "\n",
    "    generation_output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        do_sample=True,\n",
    "        max_new_tokens=max_gen_len,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "    );\n",
    "\n",
    "    new_tokens = generation_output[0][input_ids.shape[-1]:]\n",
    "    new_assistant_response = tokenizer.decode(new_tokens, skip_special_tokens=True).strip();\n",
    "\n",
    "    return new_assistant_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_wrapped(text):\n",
    "    # Regular expression pattern to detect code blocks\n",
    "    code_pattern = r'```(.+?)```'\n",
    "    matches = list(re.finditer(code_pattern, text, re.DOTALL))\n",
    "\n",
    "    if not matches:\n",
    "        # If there are no code blocks, display the entire text as Markdown\n",
    "        display(Markdown(text))\n",
    "        return\n",
    "\n",
    "    start = 0\n",
    "    for match in matches:\n",
    "        # Display the text before the code block as Markdown\n",
    "        before_code = text[start:match.start()].strip()\n",
    "        if before_code:\n",
    "            display(Markdown(before_code))\n",
    "\n",
    "        # Display the code block\n",
    "        code = match.group(0).strip()  # Extract code block\n",
    "        display(Markdown(code))  # Display code block\n",
    "\n",
    "        start = match.end()\n",
    "\n",
    "    # Display the text after the last code block as Markdown\n",
    "    after_code = text[start:].strip()  # Text after the last code block\n",
    "    if after_code:\n",
    "        display(Markdown(after_code))\n",
    "\n",
    "dialog_history = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
    "\n",
    "button = widgets.Button(description=\"Send\")\n",
    "upload_button = widgets.Button(description=\"Upload .txt or .pdf\")\n",
    "text = widgets.Textarea(layout=widgets.Layout(width='800px'))\n",
    "\n",
    "output_log = widgets.Output()\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    user_input = text.value\n",
    "    dialog_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    text.value = ''\n",
    "\n",
    "    # Change button description and color, and disable it\n",
    "    button.description = 'Processing...'\n",
    "    button.style.button_color = '#ff6e00'  # Use hex color codes for better color choices\n",
    "    button.disabled = True  # Disable the button when processing\n",
    "\n",
    "    with output_log:\n",
    "        clear_output()\n",
    "        for message in dialog_history:\n",
    "            print_wrapped(f'**{message[\"role\"].capitalize()}**: {message[\"content\"]}\\n')\n",
    "\n",
    "    assistant_response = generate_response([dialog_history]);\n",
    "\n",
    "    # Re-enable the button, reset description and color after processing\n",
    "    button.description = 'Send'\n",
    "    button.style.button_color = 'lightgray'\n",
    "    button.disabled = False\n",
    "\n",
    "    dialog_history.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "\n",
    "    with output_log:\n",
    "        clear_output()\n",
    "        for message in dialog_history:\n",
    "            print_wrapped(f'**{message[\"role\"].capitalize()}**: {message[\"content\"]}\\n')\n",
    "\n",
    "button.on_click(on_button_clicked)\n",
    "\n",
    "# Create an output widget for alerts\n",
    "alert_out = widgets.Output()\n",
    "\n",
    "clear_button = widgets.Button(description=\"Clear Chat\")\n",
    "text = widgets.Textarea(layout=widgets.Layout(width='800px'))\n",
    "\n",
    "def on_clear_button_clicked(b):\n",
    "    # Clear the dialog history\n",
    "    dialog_history.clear()\n",
    "    # Add back the initial system prompt\n",
    "    dialog_history.append({\"role\": \"system\", \"content\": SYSTEM_PROMPT})\n",
    "    # Clear the output log\n",
    "    with output_log:\n",
    "        clear_output()\n",
    "\n",
    "clear_button.on_click(on_clear_button_clicked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path input for .txt and .pdf files\n",
    "file_path_input = widgets.Text(\n",
    "    placeholder=\"Enter the path of your file here\",\n",
    "    description=\"File Path:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "# Process File Button\n",
    "process_file_button = widgets.Button(\n",
    "    description=\"Process File\",\n",
    "    disabled=True  # Initially disabled\n",
    ")\n",
    "\n",
    "# Function to check if the file path is valid\n",
    "def is_valid_file_path(file_path):\n",
    "    return os.path.isfile(file_path) and file_path.split('.')[-1] in ['txt', 'pdf']\n",
    "\n",
    "# Function to enable the process file button based on the file path validity\n",
    "def on_file_path_change(change):\n",
    "    process_file_button.disabled = not is_valid_file_path(change.new)\n",
    "\n",
    "file_path_input.observe(on_file_path_change, names='value')\n",
    "\n",
    "# Function to handle the file processing button click\n",
    "def on_process_file_button_clicked(b):\n",
    "    file_path = file_path_input.value\n",
    "    if is_valid_file_path(file_path):\n",
    "        file_type = file_path.split('.')[-1]\n",
    "        try:\n",
    "            if file_type == 'txt':\n",
    "                with open(file_path, 'r') as file:\n",
    "                    file_content = file.read()\n",
    "            elif file_type == 'pdf':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    pdf_reader = PyPDF2.PdfFileReader(file)\n",
    "                    file_content = ''.join([pdf_reader.getPage(page).extractText() for page in range(pdf_reader.numPages)])\n",
    "            \n",
    "            # Add file content to dialog history as user input\n",
    "            dialog_history.append({\"role\": \"user\", \"content\": file_content})\n",
    "\n",
    "            with output_log:\n",
    "                clear_output()\n",
    "                for message in dialog_history:\n",
    "                    print_wrapped(f'**{message[\"role\"].capitalize()}**: {message[\"content\"]}\\n')\n",
    "\n",
    "            # Generate response from the model\n",
    "            assistant_response = generate_response([dialog_history])\n",
    "\n",
    "            # Append the model's response to the dialog history\n",
    "            dialog_history.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "\n",
    "            # Display the updated dialog history\n",
    "            with output_log:\n",
    "                clear_output()\n",
    "                for message in dialog_history:\n",
    "                    print_wrapped(f'**{message[\"role\"].capitalize()}**: {message[\"content\"]}\\n')\n",
    "\n",
    "        except Exception as e:\n",
    "            with output_log:\n",
    "                clear_output()\n",
    "                print(f\"Error processing the file: {e}\")\n",
    "\n",
    "        # Clear the file path input for next use\n",
    "        file_path_input.value = ''\n",
    "        process_file_button.disabled = True\n",
    "    else:\n",
    "        with output_log:\n",
    "            clear_output()\n",
    "            print(\"Invalid file path or unsupported file type.\")\n",
    "\n",
    "process_file_button.on_click(on_process_file_button_clicked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "from ipywidgets import HBox, VBox\n",
    "\n",
    "# Create the title with HTML\n",
    "title = f\"<h1 style='color: #ff6e00;'>{four_bit_llama} ðŸ¤–</h1> <p>(Max context of: {max_context}. Uploaded files will be shortened to {max_doc_words} tokens)</p>\"\n",
    "\n",
    "# Assuming that output_log, alert_out, and text are other widgets or display elements...\n",
    "first_row = HBox([button, clear_button])  # Arrange these buttons horizontally\n",
    "# Adding the file upload and process file button to the layout\n",
    "file_upload_row = HBox([file_path_input, process_file_button])\n",
    "layout = VBox([output_log, alert_out, text, first_row, file_upload_row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1 style='color: #ff6e00;'>./models/llama-2-70b-hf-quantized-4bits/ ðŸ¤–</h1> <p>(Max context of: 4096. Uploaded files will be shortened to 3072 tokens)</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48e39bf23c244b2d90fd64391bf25ce0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Output(), Output(), Textarea(value='', layout=Layout(width='800px')), HBox(children=(Button(desâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML(title))  # Use HTML function to display the title\n",
    "display(layout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
